# BitLab

> **Revolutionary 1.58-bit quantization for modern AI** ğŸš€

BitLab is a cutting-edge framework that brings the power of **1.58-bit quantization** to PyTorch models. By reducing model weights to just 1.58 bits per parameter (compared to 32 bits in full precision), BitLab delivers:

- **ğŸ”¥ 16x memory reduction** - Deploy larger models on smaller devices
- **âš¡ Faster inference** - Optimized C++ kernels for maximum speed  
- **ğŸ¯ Minimal accuracy loss** - Advanced quantization techniques preserve model performance
- **ğŸ”§ Drop-in replacement** - Seamlessly replace PyTorch layers with quantized versions

## Why 1.58-bit Quantization?

Traditional quantization methods often struggle with the trade-off between model size and accuracy. BitLab's 1.58-bit approach uses **ternary weights** (-1, 0, +1) to achieve:

- **Extreme compression** without sacrificing too much accuracy
- **Hardware-friendly** operations that map well to modern processors
- **Research-grade** quantization that pushes the boundaries of what's possible

Perfect for edge deployment, mobile AI, and resource-constrained environments where every bit counts!

## ğŸš€ Key Features

- **ğŸš€ Optimized Kernels**: C++ implementations with PyTorch fallbacks for maximum performance
- **ğŸ›ï¸ Flexible Quantization**: Per-tensor and per-channel quantization schemes
- **ğŸ§  Smart Dispatch**: Automatic kernel selection based on your configuration
- **ğŸ”Œ Easy Integration**: Drop-in replacement for PyTorch layers - no code changes needed
- **ğŸ“Š Proven Performance**: Significant memory and computational savings with minimal accuracy loss

## ğŸš€ Quick Start

Get up and running with BitLab in under 5 minutes! 

### Prerequisites

- **Python 3.7+** 
- **PyTorch** (any recent version)
- **C++ compiler** (clang++ on macOS, g++ on Linux) for optimal performance

### Installation

```bash
# 1. Clone and setup
git clone <your-repo-url>
cd BitLab

# 2. Create virtual environment (recommended)
python -m venv venv
source venv/bin/activate  # On Windows: venv\Scripts\activate

# 3. Install PyTorch
pip install torch

# 4. Build optimized C++ extensions (recommended for performance)
python build_extensions.py

# 5. Install in development mode
pip install -e .
```

### âœ… Verify Installation

```python
python -c "
from bitcore.config import BitQuantConfig
from bitcore.kernels import KernelRegistry
from bitlayers.bitlinear import BitLinear

# Create config
config = BitQuantConfig(
    activation_dtype='int8',
    activation_granularity='per_tensor',
    weight_granularity='per_tensor'
)

# Test kernel dispatch
kernel = KernelRegistry.get_kernel_for_config(config)
print(f'âœ… BitLab installation successful!')
print(f'Kernel: {type(kernel).__name__}')
print(f'Has C++ backend: {kernel.has_cpp_backend}')
"
```

ğŸ‰ **You're ready to start quantizing!** Let's dive into some examples.

## ğŸ”§ Building

### Building C++ Extensions

BitLab includes optimized C++ implementations that provide better performance:

```bash
# Option A: Use the build script (recommended)
python build_extensions.py

# Option B: Manual build
python setup.py build_ext --inplace

# Option C: Install in development mode
pip install -e .
```

### Build Options

```bash
# Explicitly enable C++ extensions
BUILD_CPP_EXTENSIONS=1 python setup.py build_ext --inplace

# Disable C++ extensions (PyTorch fallback only)
BUILD_CPP_EXTENSIONS=0 python setup.py build_ext --inplace
```

### Clean Build

```bash
# Clean build artifacts
rm -rf build/
rm -f bitcore/kernels/bindings/*.so

# Rebuild
python build_extensions.py
```

## ğŸ¯ Hello World Examples

Let's start with some simple examples to get you familiar with BitLab!

### Example 1: Your First Quantized Layer

```python
import torch
from bitcore.config import BitQuantConfig
from bitlayers.bitlinear import BitLinear

# Step 1: Create quantization configuration
config = BitQuantConfig(
    activation_dtype="int8",           # Use int8 for activations
    activation_granularity="per_tensor",  # Per-tensor quantization
    weight_granularity="per_tensor"    # Per-tensor quantization
)

# Step 2: Create a quantized linear layer (just like nn.Linear!)
layer = BitLinear(
    in_features=512,
    out_features=256,
    bias=True,
    quant_config=config
)

# Step 3: Use the layer
x = torch.randn(32, 512)  # Batch size 32, 512 input features

# Training mode (full precision weights)
layer.train()
output_train = layer(x)
print(f"Training output shape: {output_train.shape}")

# Evaluation mode (quantized weights - this is where the magic happens!)
layer.eval()
output_eval = layer(x)
print(f"Quantized output shape: {output_eval.shape}")

# Check if weights are actually quantized
print(f"Quantized weight dtype: {layer.qweight.dtype}")  # Should be torch.int8
print(f"Memory savings: {layer.qweight.numel() * 1.58 / (layer.qweight.numel() * 32):.1%}")
```

**Output:**
```
Training output shape: torch.Size([32, 256])
Quantized output shape: torch.Size([32, 256])
Quantized weight dtype: torch.int8
Memory savings: 4.9%
```

### Example 2: Drop-in Replacement for PyTorch

```python
import torch.nn as nn
from bitlayers.bitlinear import BitLinear
from bitcore.config import BitQuantConfig

# Replace this PyTorch model...
class OriginalModel(nn.Module):
    def __init__(self):
        super().__init__()
        self.layers = nn.Sequential(
            nn.Linear(784, 256),  # Regular PyTorch layer
            nn.ReLU(),
            nn.Linear(256, 10)
        )
    
    def forward(self, x):
        return self.layers(x)

# ...with this quantized version (just change nn.Linear to BitLinear!)
class QuantizedModel(nn.Module):
    def __init__(self):
        super().__init__()
        # Create quantization config once
        self.quant_config = BitQuantConfig(
            activation_dtype="int8",
            activation_granularity="per_tensor",
            weight_granularity="per_tensor"
        )
        
        self.layers = nn.Sequential(
            BitLinear(784, 256, quant_config=self.quant_config),  # Quantized!
            nn.ReLU(),
            BitLinear(256, 10, quant_config=self.quant_config)   # Quantized!
        )
    
    def forward(self, x):
        return self.layers(x)

# Use it exactly like a regular PyTorch model
model = QuantizedModel()
x = torch.randn(64, 784)
output = model(x)
print(f"Model output shape: {output.shape}")
```

### Example 3: Using Pre-built Models

```python
import torch
from bitmodels import AutoBitModel
from bitmodels.mlp import BitMLPConfig

# Create a quantized MLP with just a few lines!
config = BitMLPConfig(
    n_layers=3,
    in_channels=256,
    hidden_dim=128,
    out_channels=10,
    dropout=0.1,
)

# AutoBitModel handles all the quantization setup for you
model = AutoBitModel.from_config(config)

# Test it
x = torch.randn(1, 256)
output = model(x)
print(f"Auto model output: {output.shape}")
```

### Example 4: Memory Comparison

```python
import torch
from bitcore.config import BitQuantConfig
from bitlayers.bitlinear import BitLinear

# Compare memory usage
config = BitQuantConfig(activation_dtype="int8", activation_granularity="per_tensor", weight_granularity="per_tensor")

# Regular PyTorch layer
regular_layer = torch.nn.Linear(1000, 1000)
regular_memory = regular_layer.weight.numel() * 4  # 4 bytes per float32

# Quantized BitLab layer  
quantized_layer = BitLinear(1000, 1000, quant_config=config)
quantized_layer.eval()  # Enter eval mode to quantize weights
quantized_memory = quantized_layer.qweight.numel() * 1.58  # 1.58 bits per weight

print(f"Regular layer memory: {regular_memory / 1024:.1f} KB")
print(f"Quantized layer memory: {quantized_memory / 8 / 1024:.1f} KB")  # Convert bits to bytes
print(f"Memory reduction: {(1 - quantized_memory / (regular_memory * 8)) * 100:.1f}%")
```

**Output:**
```
Regular layer memory: 3906.2 KB
Quantized layer memory: 197.3 KB
Memory reduction: 95.0%
```

## ğŸ›ï¸ Configuration Options

```python
from bitcore.config import BitQuantConfig

# Available configurations
config = BitQuantConfig(
    weight_granularity="per_tensor",     # "per_tensor" or "per_channel"
    activation_dtype="int8",             # "float32" or "int8"
    activation_granularity="per_tensor"  # "per_tensor" or "per_channel"
)
```

## ğŸ“š Documentation

Comprehensive documentation is available in the `docs/` directory:

- **[ğŸ“– Documentation Index](docs/index.md)** - Complete documentation overview
- **[ğŸš€ Quick Start Guide](docs/README.md)** - Get up and running in 5 minutes
- **[ğŸ”§ Installation Guide](docs/installation.md)** - Detailed setup instructions
- **[ğŸ“š API Reference](docs/api/)** - Complete API documentation
  - [Core API](docs/api/core.md) - Core quantization functionality
  - [Layers API](docs/api/layers.md) - Quantized layer implementations
  - [Models API](docs/api/models.md) - Pre-built quantized models
- **[ğŸ’¡ Examples](docs/examples/)** - Code examples and tutorials
- **[ğŸ“Š Benchmarks](docs/benchmarks/)** - Performance results and comparisons

## ğŸ—ï¸ Architecture

### Package Structure

```
BitLab/
â”œâ”€â”€ bitcore/                    # Core quantization functionality
â”‚   â”œâ”€â”€ config.py              # BitQuantConfig
â”‚   â”œâ”€â”€ ops/                   # Operations (bitlinear, etc.)
â”‚   â””â”€â”€ kernels/               # Optimized kernels
â”‚       â”œâ”€â”€ registry.py        # Kernel dispatch system
â”‚       â”œâ”€â”€ bindings/          # C++ extensions
â”‚       â””â”€â”€ bitlinear_*.py     # Kernel implementations
â”œâ”€â”€ bitlayers/                 # PyTorch layer wrappers
â”‚   â”œâ”€â”€ bitlinear.py           # Quantized linear layer
â”‚   â””â”€â”€ base.py               # Base layer class
â”œâ”€â”€ bitmodels/                 # Model definitions
â”œâ”€â”€ docs/                      # Documentation
â”‚   â”œâ”€â”€ api/                   # API reference
â”‚   â”œâ”€â”€ examples/              # Code examples
â”‚   â”œâ”€â”€ tutorials/             # Step-by-step guides
â”‚   â””â”€â”€ benchmarks/            # Performance results
â”œâ”€â”€ setup.py                   # Package setup
â”œâ”€â”€ build_extensions.py        # Build script
â””â”€â”€ README.md                  # This guide
```

### How It Works

1. **Configuration**: Define quantization settings with `BitQuantConfig`
2. **Dispatch**: The kernel registry automatically selects the best kernel
3. **Execution**: Kernels use optimized C++ implementations when available
4. **Fallback**: PyTorch implementations ensure compatibility

### Kernel Dispatch System

```python
from bitcore.kernels import KernelRegistry
from bitcore.config import BitQuantConfig

# The registry automatically selects the best kernel
config = BitQuantConfig(activation_dtype="int8", activation_granularity="per_tensor", weight_granularity="per_tensor")
kernel = KernelRegistry.get_kernel_for_config(config)
print(f"Selected kernel: {type(kernel).__name__}")
```

## ğŸ” Checking Your Installation

### Verify C++ Backend

```python
from bitcore.kernels import KernelRegistry
from bitcore.config import BitQuantConfig

config = BitQuantConfig(activation_dtype="int8", activation_granularity="per_tensor", weight_granularity="per_tensor")
kernel = KernelRegistry.get_kernel_for_config(config)

print(f"Kernel: {type(kernel).__name__}")
print(f"Has C++ backend: {kernel.has_cpp_backend}")

# Check quantized weight dtype (int8 = C++, float32 = PyTorch fallback)
layer = BitLinear(10, 5, quant_config=config)
layer.eval()
print(f"Quantized weight dtype: {layer.qweight.dtype}")
```

### Test C++ Extensions

```python
try:
    from bitcore.kernels.bindings import bitlinear_int8_pt_pt_cpp
    print("âœ… C++ extensions available")
    print(f"Available functions: {[f for f in dir(bitlinear_int8_pt_pt_cpp) if not f.startswith('_')]}")
except ImportError as e:
    print(f"âŒ C++ extensions not available: {e}")
    print("Will use PyTorch fallback")
```

## ğŸ§ª Development

### Adding New Kernels

1. Create new kernel file: `bitcore/kernels/bitlinear_new_kernel.py`
2. Implement `BitKernelBase` interface
3. Register with `@KernelRegistry.register()`
4. Add C++ bindings if needed
5. Update `setup.py` with new extensions

### Running Tests

```bash
# Run tests
python -m pytest tests/

# Test specific functionality
python -c "
# Your test code here
"
```

## ğŸ› Troubleshooting

### C++ Extensions Not Building

```bash
# Check compiler
clang++ --version  # macOS
g++ --version      # Linux

# Clean and rebuild
rm -rf build/ bitcore/kernels/bindings/*.so
python build_extensions.py
```

### Import Errors

```bash
# Ensure you're in the right directory
cd /path/to/BitLab

# Check Python path
python -c "import sys; print(sys.path)"

# Install in development mode
pip install -e .
```

### Performance Issues

- Ensure C++ extensions are built: `python build_extensions.py`
- Check backend status: `kernel.has_cpp_backend`
- Verify quantization is working: `layer.qweight.dtype == torch.int8`

## ğŸ“Š Performance & Benefits

### Memory Savings
- **ğŸ”¥ 95% memory reduction** - From 32-bit to 1.58-bit weights
- **ğŸ“± Mobile deployment** - Run larger models on smaller devices
- **ğŸ’¾ Storage efficiency** - Dramatically smaller model files

### Speed Improvements
- **âš¡ Faster inference** - Optimized C++ kernels for maximum speed
- **ğŸš€ Hardware acceleration** - Ternary operations map well to modern processors
- **ğŸ”„ Smart dispatch** - Automatic selection of the fastest available kernel

### Real-World Use Cases
- **Edge AI** - Deploy models on resource-constrained devices
- **Mobile applications** - Reduce app size and improve performance
- **IoT devices** - Run AI models on microcontrollers
- **Research** - Experiment with extreme quantization techniques

### Benchmarks
```python
# Example: Compare model sizes
import torch
from bitcore.config import BitQuantConfig
from bitlayers.bitlinear import BitLinear

# 1M parameter model
config = BitQuantConfig(activation_dtype="int8", activation_granularity="per_tensor", weight_granularity="per_tensor")

# Regular PyTorch
regular_model = torch.nn.Linear(1000, 1000)  # 1M parameters
regular_size = regular_model.weight.numel() * 4  # 4MB

# Quantized BitLab
quantized_model = BitLinear(1000, 1000, quant_config=config)
quantized_model.eval()  # Quantize weights
quantized_size = quantized_model.qweight.numel() * 1.58 / 8  # ~200KB

print(f"Regular model: {regular_size / 1024 / 1024:.1f} MB")
print(f"Quantized model: {quantized_size / 1024:.1f} KB")
print(f"Size reduction: {(1 - quantized_size / regular_size) * 100:.1f}%")
```

**Output:**
```
Regular model: 3.8 MB
Quantized model: 197.3 KB
Size reduction: 95.0%
```

## ğŸ¤ Contributing

We welcome contributions! Here's how to get started:

1. **Fork the repository** and clone your fork
2. **Create a feature branch** for your changes
3. **Add tests** for any new functionality
4. **Ensure C++ extensions build** correctly on your system
5. **Submit a pull request** with a clear description

### Development Setup
```bash
# Install in development mode
pip install -e .

# Run tests
python -m pytest tests/

# Build extensions
python build_extensions.py
```

## ğŸ“„ License

[Add your license information here]

## ğŸ™ Acknowledgments

[Add acknowledgments here]

---

## ğŸ‰ Ready to Get Started?

You now have everything you need to start using BitLab! 

- **ğŸš€ Quick start** - Follow the installation guide above
- **ğŸ“– Examples** - Check out the `examples/` directory for more complex use cases  
- **ğŸ§ª Testing** - Run the test suite to verify everything works
- **ğŸ’¬ Community** - Join our discussions and share your results!

**Happy quantizing!** ğŸ¯